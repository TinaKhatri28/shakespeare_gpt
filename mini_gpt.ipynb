{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO9bDXka9wDmR3Lddo0h+lW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TinaKhatri28/shakespeare_gpt/blob/main/mini_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt_model.py\n",
        "%%writefile gpt_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "block_size = 8\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "#LOAD DATA\n",
        "with open(\"/content/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    return \"\".join([itos[i] for i in l])\n",
        "\n",
        "#MODEL\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (k.size(-1) ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // num_heads\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(n_head)\n",
        "        self.ffwd = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok = self.token_embedding(idx)\n",
        "        pos = self.position_embedding(torch.arange(T, device=idx.device))\n",
        "        x = tok + pos\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits = logits.view(B * T, -1)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.8, top_k=40):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, ix = torch.topk(logits, top_k)\n",
        "                logits_filtered = torch.full_like(logits, float(\"-inf\"))\n",
        "                logits_filtered.scatter_(1, ix, v)\n",
        "                logits = logits_filtered\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "def generate_shakespeare_text(\n",
        "    model, start_text=\"ROMEO:\", max_tokens=300, temperature=0.6, top_k=20\n",
        "):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    context = torch.tensor([encode(start_text)], dtype=torch.long).to(device)\n",
        "    output = model.generate(\n",
        "        context,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "    )\n",
        "    return decode(output[0].tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyJRoJuWTAJn",
        "outputId": "0f937706-b336-4d5f-fed4-d483412bcb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gpt_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "block_size = 8\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# LOAD DATA FOR VOCAB\n",
        "with open(\"/content/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(l):\n",
        "    return \"\".join([itos[i] for i in l])\n",
        "\n",
        "# MODEL\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * (k.size(-1) ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // num_heads\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.dropout(self.proj(out))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(n_head)\n",
        "        self.ffwd = FeedForward()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok = self.token_embedding(idx)\n",
        "        pos = self.position_embedding(torch.arange(T, device=idx.device))\n",
        "        x = tok + pos\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            logits = logits.view(B * T, -1)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.8, top_k=40):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, ix = torch.topk(logits, top_k)\n",
        "                logits_filtered = torch.full_like(logits, float(\"-inf\"))\n",
        "                logits_filtered.scatter_(1, ix, v)\n",
        "                logits = logits_filtered\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "\n",
        "def generate_shakespeare_text(\n",
        "    model, start_text=\"ROMEO:\", max_tokens=300, temperature=0.6, top_k=20\n",
        "):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    context = torch.tensor([encode(start_text)], dtype=torch.long).to(device)\n",
        "    output = model.generate(\n",
        "        context,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "    )"
      ],
      "metadata": {
        "id": "KGnvWJj1kEh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from gpt_model import GPTLanguageModel, encode, decode\n"
      ],
      "metadata": {
        "id": "0WOgOxytTDfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 8\n",
        "learning_rate = 1e-3\n",
        "max_iters = 50000\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "omfoXi5gTg-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "R-zpnpBATj-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ],
      "metadata": {
        "id": "uBQ0Mom-Tyki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for step in range(max_iters):\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "        print(f\"step {step} | loss {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z8y5VQKTz7i",
        "outputId": "93f0d826-121d-4165-f7c2-e0f4f1862efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 | loss 4.3525\n",
            "step 1000 | loss 2.0220\n",
            "step 2000 | loss 1.8312\n",
            "step 3000 | loss 1.9804\n",
            "step 4000 | loss 1.7582\n",
            "step 5000 | loss 1.8314\n",
            "step 6000 | loss 1.7698\n",
            "step 7000 | loss 1.8299\n",
            "step 8000 | loss 1.6882\n",
            "step 9000 | loss 1.7216\n",
            "step 10000 | loss 1.6928\n",
            "step 11000 | loss 1.8069\n",
            "step 12000 | loss 1.8366\n",
            "step 13000 | loss 1.7346\n",
            "step 14000 | loss 1.7920\n",
            "step 15000 | loss 1.7860\n",
            "step 16000 | loss 1.6883\n",
            "step 17000 | loss 1.7947\n",
            "step 18000 | loss 1.7345\n",
            "step 19000 | loss 1.7723\n",
            "step 20000 | loss 1.6783\n",
            "step 21000 | loss 1.7849\n",
            "step 22000 | loss 1.7322\n",
            "step 23000 | loss 1.6834\n",
            "step 24000 | loss 1.6660\n",
            "step 25000 | loss 1.5662\n",
            "step 26000 | loss 1.7052\n",
            "step 27000 | loss 1.6643\n",
            "step 28000 | loss 1.6521\n",
            "step 29000 | loss 1.7077\n",
            "step 30000 | loss 1.6222\n",
            "step 31000 | loss 1.7901\n",
            "step 32000 | loss 1.6690\n",
            "step 33000 | loss 1.7507\n",
            "step 34000 | loss 1.6879\n",
            "step 35000 | loss 1.6340\n",
            "step 36000 | loss 1.6442\n",
            "step 37000 | loss 1.7961\n",
            "step 38000 | loss 1.6570\n",
            "step 39000 | loss 1.6389\n",
            "step 40000 | loss 1.7476\n",
            "step 41000 | loss 1.6148\n",
            "step 42000 | loss 1.6713\n",
            "step 43000 | loss 1.6314\n",
            "step 44000 | loss 1.7484\n",
            "step 45000 | loss 1.7513\n",
            "step 46000 | loss 1.7768\n",
            "step 47000 | loss 1.7348\n",
            "step 48000 | loss 1.7608\n",
            "step 49000 | loss 1.6922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"shakespeare_gpt.pt\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "id": "oIzgzZcxBra1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(generate_shakespeare_text(model, \"ROMEO:\", 300))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_z1EzhSeUl5",
        "outputId": "fd671218-a48d-4cab-acb4-c0ee2cdc5b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "model.load_state_dict(torch.load(\"shakespeare_gpt.pt\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "output = model.generate(\n",
        "    context,\n",
        "    max_new_tokens=600,\n",
        "    temperature=0.5,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(decode(output[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq76MytftFRz",
        "outputId": "c0ff913c-9174-437b-9478-2ef9e220398d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "By your highness of the house is in the riar a many back a heads with the wars, and not a mistress'd our gentleman:\n",
            "The world the many father,\n",
            "Which will be so,\n",
            "And but the want may for a man of Englanded many father father, the great sun a prayers and the warrant the mind of York, I will the while I may be a face;\n",
            "With the many should the come, whose the corn to such a subjects that I will be so not hands and respect up the prove and the world the liberty,\n",
            "That the people to the earth her first thou was see you are did great me should be in the banished, with her be to for the state here to b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du96V338tzUf",
        "outputId": "599c6a18-77a8-46db-b049-428b72895822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.52.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.52.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st"
      ],
      "metadata": {
        "id": "vgrqL98ot25m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.title(\"Shakespeare GPT Text Generator\")\n",
        "st.write(\"Generate Shakespeare-style text using your trained GPT model.\")\n",
        "\n",
        "start_text = st.text_input(\"Start your text\", value=\"KING RICHARD III: \")\n",
        "max_tokens = st.slider(\"Maximum tokens to generate\", min_value=50, max_value=1000, value=400, step=50)\n",
        "temperature = st.slider(\"Temperature\", min_value=0.1, max_value=1.5, value=0.5, step=0.1)\n",
        "top_k = st.slider(\"Top-K Sampling\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"Generate Text\"):\n",
        "    with st.spinner(\"Generating Shakespearean text...\"):\n",
        "        generated_text = generate_shakespeare_text(\n",
        "            model,\n",
        "            start_text=start_text,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k\n",
        "        )\n",
        "    st.text_area(\"Generated Text\", value=generated_text, height=400)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0gm0pMMtQvi",
        "outputId": "cd35f3c4-9d54-4eb1-b280-87531217d475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-12-14 07:16:20.719 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.083 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-12-14 07:16:21.086 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.089 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.090 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.092 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.093 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.097 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.099 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.102 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.104 Session state does not function when running a script without `streamlit run`\n",
            "2025-12-14 07:16:21.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.109 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.119 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.122 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-12-14 07:16:21.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok -q"
      ],
      "metadata": {
        "id": "li2XCn30upIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Shakespeare GPT Text Generator \")\n",
        "st.write(\"Generate Shakespeare-style text using your trained GPT model.\")\n",
        "\n",
        "start_text = st.text_input(\"Start your text\", value=\"KING RICHARD III: \")\n",
        "max_tokens = st.slider(\"Maximum tokens to generate\", min_value=50, max_value=500, value=200)\n",
        "temperature = st.slider(\"Temperature\", min_value=0.1, max_value=1.5, value=1.0)\n",
        "top_k = st.slider(\"Top-k Sampling\", min_value=1, max_value=100, value=50, step=1)\n",
        "\n",
        "if st.button(\"Generate Text\"):\n",
        "    with st.spinner(\"Generating Shakespearean text...\"):\n",
        "        generated_text = generate_shakespeare_text(\n",
        "            model,\n",
        "            start_text=start_text,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k\n",
        "        )\n",
        "        st.text_area(\"Generated Text\", value=generated_text, height=400)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl5yIVTSu0ta",
        "outputId": "c8c534f8-80e7-463f-803a-7d614fa61e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install cloudflared\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "# Kill any existing streamlit processes\n",
        "!pkill streamlit\n",
        "\n",
        "# Start streamlit in background\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "\n",
        "# Wait for streamlit to start\n",
        "import time\n",
        "time.sleep(5)\n",
        "\n",
        "# Create tunnel\n",
        "!nohup cloudflared tunnel --url http://localhost:8501 > /content/tunnel.log 2>&1 &\n",
        "\n",
        "# Wait and show URL\n",
        "time.sleep(5)\n",
        "!grep -o 'https://.*\\.trycloudflare.com' /content/tunnel.log | head -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCZPQxAfvQdF",
        "outputId": "68eaa2ac-66d6-46d4-d609-60128b077477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 121693 files and directories currently installed.)\n",
            "Preparing to unpack cloudflared-linux-amd64.deb ...\n",
            "Unpacking cloudflared (2025.11.1) over (2025.11.1) ...\n",
            "Setting up cloudflared (2025.11.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from gpt_model import GPTLanguageModel, generate_shakespeare_text\n",
        "\n",
        "#STREAMLIT APP\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = GPTLanguageModel()\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('/content/shakespeare_gpt.pt', map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        return model, True, device\n",
        "    except Exception as e:\n",
        "        return None, False, str(e)\n",
        "\n",
        "st.title(\"Shakespeare GPT Text Generator\")\n",
        "st.write(\"Shakespeare style text\")\n",
        "\n",
        "model, model_loaded, info = load_model()\n",
        "\n",
        "if model_loaded:\n",
        "    st.success(f\"Loading Model on {info}!\")\n",
        "\n",
        "    start_text = st.text_input(\"Start your text\", value=\"KING RICHARD III: \")\n",
        "    max_tokens = st.slider(\"Maximum tokens to generate\", min_value=50, max_value=500, value=200)\n",
        "    temperature = st.slider(\"Temperature\", min_value=0.1, max_value=1.5, value=0.8, step=0.1)\n",
        "    top_k = st.slider(\"Top-k Sampling\", min_value=1, max_value=100, value=40, step=1)\n",
        "\n",
        "    if st.button(\"Generate Text\", type=\"primary\"):\n",
        "        with st.spinner(\"Generating Shakespearean text...\"):\n",
        "            try:\n",
        "                generated_text = generate_shakespeare_text(\n",
        "                    model,\n",
        "                    start_text=start_text,\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=temperature,\n",
        "                    top_k=top_k\n",
        "                )\n",
        "                st.text_area(\"Generated Text\", value=generated_text, height=400)\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error: {str(e)}\")\n",
        "else:\n",
        "    st.error(f\" Failed to load model: {info}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIdeNkDWwvOV",
        "outputId": "d57fda44-c36c-41bb-c46d-5c4c73bff11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill streamlit\n",
        "!pkill cloudflared\n",
        "\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "time.sleep(5)\n",
        "!nohup cloudflared tunnel --url http://localhost:8501 > /content/tunnel.log 2>&1 &\n",
        "time.sleep(5)\n",
        "!grep -o 'https://.*\\.trycloudflare.com' /content/tunnel.log | head -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px8W8NVCwy8P",
        "outputId": "2badeda1-6602-4e51-efce-686c0f669f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://safari-acquire-regime-lexmark.trycloudflare.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIa7zuraprbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}